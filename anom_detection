import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras import layers, models

# Example data: Each column is a sensor, and each row is a time step
data = pd.DataFrame({
    'sensor_1': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'C', 'B', 'A'],
    'sensor_2': ['X', 'Y', 'Z', 'X', 'Z', 'Y', 'X', 'Y', 'X', 'Z'],
    'sensor_3': ['P', 'Q', 'R', 'P', 'Q', 'R', 'Q', 'R', 'P', 'P']
})

# Convert categorical data into numerical encoding (letters to numbers)
label_encoders = {}
for col in data.columns:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

# Define a sliding window size
window_size = 5

# Function to create sequences from time series data
def create_sequences(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data.iloc[i:i+window_size].values)
        y.append(data.iloc[i+window_size].values)  # Predict the next time step
    return np.array(X), np.array(y)

# Prepare sequences for model (3D: [samples, time_steps, features])
X, y = create_sequences(data, window_size)

print("Data shape (X):", X.shape)  # (samples, time_steps, features)
print("Labels shape (y):", y.shape)  # (samples, features)


# Define the Masked Word Prediction Model (using Feed-Forward layers)
def create_masked_word_model(vocab_size, embedding_dim, window_size):
    inputs = layers.Input(shape=(window_size,))  # Input for window of 5 values (word)
    embedding_layer = layers.Embedding(vocab_size, embedding_dim)(inputs)
    
    # Feed-forward layers instead of LSTM
    x = layers.Flatten()(embedding_layer)  # Flatten the embeddings before passing to dense layers
    x = layers.Dense(128, activation='relu')(x)  # First dense layer
    x = layers.Dense(64, activation='relu')(x)  # Second dense layer
    x = layers.Dense(32, activation='relu')(x)  # Third dense layer
    output = layers.Dense(vocab_size, activation='softmax')(x)  # Output layer to predict masked word
    model = models.Model(inputs=inputs, outputs=output)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
    return model

# Define the size of the vocabulary and the embedding dimension
vocab_size = len(np.unique(data.values))
embedding_dim = 50  # Choose an embedding dimension

# Create and train the model
masked_model = create_masked_word_model(vocab_size, embedding_dim, window_size)

# Function to create masked input for training the masked word prediction model
def create_masked_input(X, mask_rate=0.2):
    masked_X = X.copy()
    for i in range(masked_X.shape[0]):
        mask = np.random.rand(*masked_X[i].shape) < mask_rate
        masked_X[i][mask] = -1  # Masking by setting to a value that doesn't exist in the data
    return masked_X

# Create masked inputs and train the model
masked_X = create_masked_input(X)

# Train the masked word prediction model
masked_model.fit(masked_X, y, epochs=10, batch_size=32, validation_split=0.2)


# Function to predict anomalies based on forecasting errors
def predict_anomalies(forecasting_model, X_embeddings, y, threshold_percentile=95):
    # Predict the next sequence using the forecasting model
    predictions = forecasting_model.predict(X_embeddings)
    
    # Calculate the prediction errors (mean squared error)
    errors = np.mean(np.abs(predictions - y), axis=1)
    
    # Set a threshold for anomaly detection (95th percentile of errors)
    threshold = np.percentile(errors, threshold_percentile)
    
    # Mark anomalies based on the error threshold
    anomalies = errors > threshold
    
    # Print the anomaly scores and the corresponding anomalies
    for i in range(len(anomalies)):
        print(f"Time Step {i + window_size}: Anomaly Score = {errors[i]}, Anomaly = {anomalies[i]}")
    
    return anomalies, errors


# LSTM model for forecasting the next sequence (using embeddings)
def create_lstm_forecasting_model(window_size, embedding_dim, vocab_size):
    inputs = layers.Input(shape=(window_size, embedding_dim))  # Input for sequence of embeddings
    x = layers.LSTM(64, return_sequences=True)(inputs)
    x = layers.LSTM(64)(x)
    output = layers.Dense(vocab_size)(x)  # Output layer predicts next time step
    model = models.Model(inputs=inputs, outputs=output)
    model.compile(optimizer='adam', loss='mse')
    return model

# Create the LSTM model for forecasting (using the embeddings from the masked model)
forecasting_model = create_lstm_forecasting_model(window_size, embedding_dim, vocab_size)

# Use the embeddings generated by the masked word model as input to the LSTM
X_embeddings = masked_model.predict(masked_X)  # Use embeddings from the masked word model

# Train the LSTM on the embeddings
forecasting_model.fit(X_embeddings, y, epochs=10, batch_size=32, validation_split=0.2)

# Predict anomalies based on the forecast errors
anomalies, errors = predict_anomalies(forecasting_model, X_embeddings, y, threshold_percentile=95)
